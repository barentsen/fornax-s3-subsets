{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae8855",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "from cytoolz import groupby, merge\n",
    "from cytoolz.curried import get, get_in\n",
    "from killscreen import subutils, shortcuts as ks\n",
    "from killscreen.aws import ec2\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv\n",
    "from gPhoton.pretty import make_monitors\n",
    "from more_itertools import distribute\n",
    "from pyarrow import parquet\n",
    "\n",
    "from subset.utilz.mount_s3 import mount_bucket\n",
    "from subset.science.ps1_utils import prune_ps1_catalog\n",
    "from subset.utilz.generic import parse_topline, sample_table\n",
    "\n",
    "key = \"/home/ubuntu/galex_swarm.pem\"\n",
    "uname = \"ubuntu\"\n",
    "DUMP_PATH = '/home/ubuntu/.slice_test/'\n",
    "os.makedirs(DUMP_PATH, exist_ok=True)\n",
    "S3_ROOT = \"/mnt/s3\"\n",
    "BUCKET=\"nishapur\"\n",
    "# mount bucket to fetch metadata\n",
    "mount_bucket(backend=\"goofys\", mount_path=S3_ROOT, bucket=BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0c4ad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize a killscreen Cluster\n",
    "descriptions = ec2.describe(\n",
    "    tag_filters={'Name': 'fornax-slice'}, states=(\"running\", \"stopped\")\n",
    ")\n",
    "# ...either from already-running EC2 instances...\n",
    "if len(descriptions) == 0:\n",
    "    cluster = ec2.Cluster.launch(\n",
    "        count=4,\n",
    "        template=\"fornax-slice\", \n",
    "        key=key, \n",
    "        uname=uname, \n",
    "        use_private_ip=True\n",
    "    )\n",
    "# ...or from a new fleet request.\n",
    "else:\n",
    "    cluster = ec2.Cluster.from_descriptions(\n",
    "        descriptions, key=key, uname=uname, use_private_ip=True\n",
    "    )\n",
    "    cluster.start()\n",
    "    [instance.wait_until_running() for instance in cluster.instances]\n",
    "    cluster.add_keys()\n",
    "    print(\"\\n\".join([str(i) for i in cluster.instances]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0622b39",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# freshen these instances\n",
    "def git_update(*repo_names):\n",
    "    return ks.chain(\n",
    "        [f\"cd {repo}; git clean -d -fx; git pull & cd ~\" for repo in repo_names], \"and\"\n",
    "    )\n",
    "update = git_update(\"fornax-s3-subsets\", \"killscreen\", \"gphoton_working\")\n",
    "updaters = cluster.command(update, _bg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c3039",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set up metadata objects in order to pick targets for slicing\n",
    "\n",
    "# catalog of PS1 extragalactic extended objects, including explicit\n",
    "# assignments to PS1 stack image projection / sky cells and GALEX \n",
    "# eclipse numbers (not used here)\n",
    "catalog_fn = \"ps1_extragalactic_skycells_eclipses.parquet\"\n",
    "if not Path(catalog_fn).exists():\n",
    "    shutil.copy(\n",
    "        Path(S3_ROOT, \"ps1/metadata\", catalog_fn),\n",
    "        Path(catalog_fn)\n",
    "    )\n",
    "catalog = parquet.read_table(catalog_fn)\n",
    "\n",
    "# for this demo, we only staged a subset of those PS1 stack images \n",
    "# (all of them at all 5 bands would be > 80 TB). this is a list of \n",
    "# the (randomly selected) projection and sky cells we staged.\n",
    "test_cell_fn = \"ps1_extragalactic_skycells_eclipses_1k_cell_subset.csv\"\n",
    "arbitrary_test_cells = (\n",
    "    pa.csv.read_csv(Path(S3_ROOT, \"ps1/metadata\", test_cell_fn))\n",
    "    .cast(pa.schema([(\"proj_cell\", pa.uint16()), (\"sky_cell\", pa.uint8())]))\n",
    ")\n",
    "small_catalog = prune_ps1_catalog(catalog, arbitrary_test_cells)\n",
    "del catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28df1b3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# various settings for the test\n",
    "\n",
    "# how many objects shall we collect slices for? (785510 are available in this test set)\n",
    "TARGET_COUNT = 2000\n",
    "# optional parameter -- restrict the total number of PS1 source cells to test the \n",
    "# performance effects of denser sampling.\n",
    "# (1000 total PS1 cells are available in this test set).\n",
    "# note that the total number of images accessed is number of cells * number of bands.\n",
    "MAX_CELL_COUNT = 100\n",
    "if MAX_CELL_COUNT is not None:\n",
    "    test_catalog = prune_ps1_catalog(\n",
    "        small_catalog, sample_table(arbitrary_test_cells, k=MAX_CELL_COUNT)\n",
    "    )\n",
    "else:\n",
    "    test_catalog = small_catalog\n",
    "targets = sample_table(test_catalog, k=TARGET_COUNT).to_pylist()\n",
    "\n",
    "# split these into chunks of work, making sure that all targets within\n",
    "# a single cell / image are assigned to the same instance --\n",
    "target_groups = groupby(get(['proj_cell', 'sky_cell']), targets)\n",
    "# this is a simple heuristic to distribute work evenly, given the above constraint:\n",
    "groups = sorted(target_groups.values(), key=lambda v: 1 / len(v))\n",
    "work_chunks = [\n",
    "    tuple(chain.from_iterable(chunk)) \n",
    "    for chunk in distribute(len(cluster.instances), groups)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3fd53",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# what script / interpreter are we actually using on the remote instances\n",
    "env = cluster.instances[0].conda_env(\"fornax-slice-testing\")\n",
    "python = f\"{env}/bin/python\"\n",
    "endpoint = \"/home/ubuntu/fornax-s3-subsets/code/ps1_cutout_endpoint.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f433a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# simple process join function\n",
    "def wait_on(processes, polling_delay=0.1):\n",
    "    while any([p.is_alive() for p in processes]):\n",
    "        time.sleep(polling_delay)\n",
    "\n",
    "# when a remote process is done, grab the files from that instance\n",
    "# this could be done more concurrently, but synchronizing is a pain.\n",
    "# maybe scp from remotes? ideally inside the dump loop.\n",
    "getters = []\n",
    "def grab_when_done(process, *_):\n",
    "    print(f\"{process.host.ip} done; getting files\")\n",
    "    getter = process.host.get(f\"{DUMP_PATH}*\", DUMP_PATH, _bg=True)\n",
    "    getters.append(getter)\n",
    "        \n",
    "# delete everything local so as to avoid confusion\n",
    "subutils.run(f\"rm {DUMP_PATH}/* &\")\n",
    "\n",
    "# set up some basic benchmarking...\n",
    "stat, note = make_monitors(silent=True)\n",
    "# ...and initiate the remote processes\n",
    "remote_processes = []\n",
    "for chunk, instance in zip(work_chunks, cluster.instances):\n",
    "    command = f\"{python} {endpoint} '{chunk}'\"\n",
    "    viewer = instance.command(\n",
    "        command, _bg=True, _viewer=True, _done=grab_when_done\n",
    "    )\n",
    "    remote_processes.append(viewer)\n",
    "wait_on(remote_processes)\n",
    "note(f\"remote processes completed,{stat()}\", True)\n",
    "wait_on(getters)\n",
    "note(f\"cleaned up files from remotes,{stat()}\", True)\n",
    "\n",
    "retrieved_dumps = os.listdir(DUMP_PATH)\n",
    "\n",
    "cutfiles = tuple(filter(lambda f: f.endswith(\"pkl\"), retrieved_dumps))\n",
    "note(f\"got {len(targets) * 2} cuts,{stat(total=True)}\", True)\n",
    "log = note(None, eject=True)\n",
    "rate, weight = parse_topline(log)\n",
    "print(f\"{rate} cutouts/s, {weight} MB / cutout (local only)\")\n",
    "\n",
    "# cleanup cached arrays on remotes\n",
    "deletions = cluster.command(f\"rm {DUMP_PATH}/* &\", _bg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfa8c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# should you like: examine logs from remotes...\n",
    "import pandas as pd\n",
    "logs = []\n",
    "for logfile in filter(lambda f: f.endswith(\"csv\"), retrieved_dumps):\n",
    "    remote_log = pd.read_csv(Path(DUMP_PATH, logfile))\n",
    "    remote_log[\"host\"] = re.search(\n",
    "        r\"(?<=ip_)(\\d+_){4}\", logfile\n",
    "    ).group(0)[:-1]\n",
    "    logs.append(remote_log)\n",
    "logs = pd.concat(logs)\n",
    "logs.columns = [\"timestamp\", \"event\", \"duration\", \"volume\", \"host\"]\n",
    "logs.sort_values(by=[\"host\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1e80c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ...or your winnings\n",
    "cuts = []\n",
    "for file in cutfiles:\n",
    "    with open(Path(DUMP_PATH, file), \"rb\") as stream:\n",
    "        cuts.append(pickle.load(stream))\n",
    "cuts = merge(cuts)\n",
    "arrays = tuple(map(get_in(['arrays', 0]), cuts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2afa14",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from random import choice\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, grid = plt.subplot_mosaic(np.arange(9).reshape(3,3))\n",
    "plt.close()\n",
    "for ax in grid.values():\n",
    "    ax.set_axis_off()\n",
    "\n",
    "for ix in grid.keys():\n",
    "    array = arrays[choice(range(len(arrays)))]\n",
    "    clipped = np.clip(array, *np.percentile(array, (1, 99)))\n",
    "    grid[ix].imshow(clipped, cmap='autumn')\n",
    "    \n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97569b7a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# destroy the cluster if you are done with it\n",
    "cluster.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}