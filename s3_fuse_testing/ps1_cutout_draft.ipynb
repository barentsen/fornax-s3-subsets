{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "710f70e2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count, Pool\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from cytoolz.curried import get\n",
    "from killscreen.monitors import Netstat, Stopwatch\n",
    "import pyarrow as pa\n",
    "from pyarrow import parquet\n",
    "import pyarrow.csv\n",
    "\n",
    "from s3_fuse.mount_s3 import mount_bucket\n",
    "from s3_fuse.ps1_utils import prune_ps1_catalog, get_ps1_cutouts, ps1_stack_path, request_ps1_cutout\n",
    "from s3_fuse.utilz import make_loaders, sample_table, parse_topline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86eed92e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 'configuration'\n",
    "\n",
    "BUCKET = 'nishapur'\n",
    "S3_ROOT = '/mnt/s3'\n",
    "\n",
    "# desired cutout side length in degrees\n",
    "CUTOUT_SIDE_LENGTH = 60 / 3600\n",
    "\n",
    "# which PS1 bands are we looking at? (currently only g and z are staged.)\n",
    "PS1_BANDS = (\"g\", \"z\")\n",
    "\n",
    "# select loaders -- options are \"astropy\", \"fitsio\", \"greedy_astropy\", \"greedy_fitsio\"\n",
    "# NOTE: because all the files this particular notebook is looking\n",
    "# at are RICE-compressed, there is unlikely to be much difference\n",
    "# between astropy and greedy_astropy -- astropy does not support\n",
    "# loading individual tiles from a a tile-compressed FITS file.\n",
    "LOADERS = make_loaders(\"fitsio\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec99697a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# mount bucket to fetch metadata\n",
    "mount_bucket(backend=\"goofys\", mount_path=S3_ROOT, bucket=BUCKET)\n",
    "\n",
    "# catalog of PS1 extragalactic extended objects, including explicit\n",
    "# assignments to PS1 stack image projection / sky cells and GALEX \n",
    "# eclipse numbers (not used here)\n",
    "catalog_fn = \"ps1_extragalactic_skycells_eclipses.parquet\"\n",
    "if not Path(catalog_fn).exists():\n",
    "    shutil.copy(\n",
    "        Path(S3_ROOT, \"ps1/metadata\", catalog_fn),\n",
    "        Path(catalog_fn)\n",
    "    )\n",
    "catalog = parquet.read_table(catalog_fn)\n",
    "\n",
    "# for this demo, we only staged a subset of those PS1 stack images \n",
    "# (all of them at all 5 bands would be > 80 TB). this is a list of \n",
    "# the (randomly selected) projection and sky cells we staged.\n",
    "test_cell_fn = \"ps1_extragalactic_skycells_eclipses_1k_cell_subset.csv\"\n",
    "arbitrary_test_cells = (\n",
    "    pa.csv\n",
    "    .read_csv(Path(S3_ROOT, \"ps1/metadata\", test_cell_fn))\n",
    "    .cast(pa.schema([(\"proj_cell\", pa.uint16()), (\"sky_cell\", pa.uint8())]))\n",
    ")\n",
    "small_catalog = prune_ps1_catalog(catalog, arbitrary_test_cells)\n",
    "del catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5ae7390",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test setttings\n",
    "\n",
    "# how many objects shall we collect slices for? (785510 are available in this test set)\n",
    "TARGET_COUNT = 100\n",
    "# optional parameter -- restrict the total number of PS1 source cells to test the \n",
    "# performance effects of denser sampling.\n",
    "# (1000 total PS1 cells are available in this test set).\n",
    "# note that the total number of images accessed is number of cells * number of bands.\n",
    "MAX_CELL_COUNT = 20\n",
    "if MAX_CELL_COUNT is not None:\n",
    "    test_catalog = prune_ps1_catalog(\n",
    "        small_catalog, sample_table(arbitrary_test_cells, k=MAX_CELL_COUNT)\n",
    "    )\n",
    "else:\n",
    "    test_catalog = small_catalog\n",
    "targets = sample_table(test_catalog, k=TARGET_COUNT).to_pylist()\n",
    "ps1_stacks = set(map(get(['proj_cell', 'sky_cell']), targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97fe8e90",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# per-loader performance-tuning parameters\n",
    "# image_chunksize: how many images shall we initialize at once?\n",
    "# image_threads: how many threads shall we init with in parallel? (None to disable.)\n",
    "# cut_threads: how many threads shall we cut with in parallel? (None to disable.)\n",
    "# note that S3 handles parallel requests very well; on a smaller instance, you will\n",
    "# usually run out of CPU or absolute input bandwidth before you exhaust its willingness to\n",
    "# serve parallel requests.\n",
    "TUNING = {\n",
    "    \"fitsio\": {\n",
    "        \"image_chunksize\": 40, \"image_threads\": cpu_count() * 7, \"cut_threads\": cpu_count() * 7\n",
    "    },\n",
    "    \"greedy_fitsio\": {\n",
    "        \"image_chunksize\": 10, \"image_threads\": cpu_count() * 2, \"cut_threads\": None\n",
    "    },\n",
    "    \"default\": {\n",
    "        \"image_chunksize\": 20, \"image_threads\": cpu_count() * 4, \"cut_threads\": cpu_count() * 4\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e0c07a2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----testing fitsio----\n",
      "initialized 38 images,1.71 s,16.18 MB\n",
      "made 200 cutouts,5.84 s,653.37 MB\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 12; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m mount_bucket(\n\u001b[1;32m      9\u001b[0m     backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoofys\u001b[39m\u001b[38;5;124m\"\u001b[39m, remount\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mount_path\u001b[38;5;241m=\u001b[39mS3_ROOT, bucket\u001b[38;5;241m=\u001b[39mBUCKET\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m tuning_params \u001b[38;5;241m=\u001b[39m TUNING[loader_name] \u001b[38;5;28;01mif\u001b[39;00m loader_name \u001b[38;5;129;01min\u001b[39;00m TUNING\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m TUNING[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m cuts, logs[loader_name] \u001b[38;5;241m=\u001b[39m \u001b[43mget_ps1_cutouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mps1_stacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCUTOUT_SIDE_LENGTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mS3_ROOT\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/ps1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPS1_BANDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtuning_params\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m rate, weight \u001b[38;5;241m=\u001b[39m parse_topline(logs[loader_name])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cutouts/s, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB / cutout\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/fornax-s3-subsets/s3_fuse_testing/s3_fuse/ps1_utils.py:193\u001b[0m, in \u001b[0;36mget_ps1_cutouts\u001b[0;34m(stacks, loader, targets, length, data_root, bands, verbose, logged, image_chunksize, image_threads, cut_threads, return_cuts, dump, dump_to)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m cut \u001b[38;5;129;01min\u001b[39;00m chunk_cuts\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    192\u001b[0m             \u001b[38;5;28;01mdel\u001b[39;00m cut[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrays\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 193\u001b[0m     cuts \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m chunk_cuts\n\u001b[1;32m    194\u001b[0m note(\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmade \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cuts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cuts from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(stacks) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(bands)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstat(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    197\u001b[0m     verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    198\u001b[0m )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cuts, note(\u001b[38;5;28;01mNone\u001b[39;00m, eject\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 12; 2 is required"
     ]
    }
   ],
   "source": [
    "logs = {}\n",
    "for loader_name, loader in LOADERS.items():\n",
    "    # remount bucket to avoid \"cheating\" -- note that this is still a little cheaty\n",
    "    # because of unreliable, unmodifiable, and entirely black-box caching on S3 side, and loaders\n",
    "    # later in the list will tend to do better. for a 'fairer' comparison, reroll\n",
    "    # between each loader.\n",
    "    print(f\"----testing {loader_name}----\")\n",
    "    mount_bucket(\n",
    "        backend=\"goofys\", remount=True, mount_path=S3_ROOT, bucket=BUCKET\n",
    "    )\n",
    "    tuning_params = TUNING[loader_name] if loader_name in TUNING.keys() else TUNING[\"default\"]\n",
    "    cuts, logs[loader_name] = get_ps1_cutouts(\n",
    "        ps1_stacks, \n",
    "        loader, \n",
    "        targets, \n",
    "        CUTOUT_SIDE_LENGTH, \n",
    "        f\"{S3_ROOT}/ps1\", \n",
    "        PS1_BANDS,\n",
    "        verbose=2,\n",
    "        **tuning_params\n",
    "    )\n",
    "    rate, weight = parse_topline(logs[loader_name])\n",
    "    print(f\"{rate} cutouts/s, {weight} MB / cutout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c9b0bc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# comparison to the PS1 cutout service. We can crank up the number of threads we're using...\n",
    "# but at some point we will essentially be attacking the service; no one else will be able to use it. \n",
    "# also note that it _also_ performs serverside caching.\n",
    "\n",
    "REQUEST_THREADS = None\n",
    "watch, netstat = Stopwatch(silent=True), Netstat()\n",
    "watch.start(), netstat.update()\n",
    "req_cutouts = {}\n",
    "request_pool = Pool(REQUEST_THREADS) if REQUEST_THREADS is not None else None\n",
    "\n",
    "for target in targets:\n",
    "    for band in PS1_BANDS:\n",
    "        args = (\n",
    "            ps1_stack_path(target['proj_cell'], target['sky_cell'], band),\n",
    "            target['ra'],\n",
    "            target['dec'],\n",
    "            CUTOUT_SIDE_LENGTH * 3600,\n",
    "            \"fits\"\n",
    "        )\n",
    "        if request_pool is None:\n",
    "            req_cutouts[target['obj_id']] = request_ps1_cutout(*args)\n",
    "        else:\n",
    "            req_cutouts[target['obj_id']] = request_pool.apply_async(\n",
    "                request_ps1_cutout, args\n",
    "            )\n",
    "if request_pool is not None:\n",
    "    req_cutouts = {\n",
    "        obj_id: result.get() for obj_id, result in req_cutouts.items()\n",
    "    }\n",
    "netstat.update()\n",
    "count = len(targets) * len(PS1_BANDS)\n",
    "sec = watch.peek()\n",
    "vol = list(netstat.total.values())[-1] / 1024 ** 2\n",
    "print(\n",
    "    f\"made {count} cutouts,{sec} total seconds,{round(vol, 2)} total MB,\\n\"\n",
    "    f\"{round(count / sec, 2)} cutouts / s,{round(vol / count, 2)} MB/cutout\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
